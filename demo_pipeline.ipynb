{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a51fab2",
   "metadata": {},
   "source": [
    "# QWEN3-VL-8B as a powerful OCR and document processor\n",
    "This notebook accompanies the [talk](https://hong-kong.aitinkerers.org/talks/rsvp_wdu0jEPpJYA) given by Marcus Leiwe at the [AI Tinkerers Hong Kong](https://hong-kong.aitinkerers.org/) meetup on the 27th November 2025.\n",
    "\n",
    "The original project solved a critical data bottleneck for a Charity client [Branches of Hope](https://branchesofhope.org.hk/): digitising thousands of handwritten and scanned forms into a queryable database without sending sensitive PII to closed-source providers (like OpenAI), thereby ensuring cost-efficiency and GDPR compliance.\n",
    "\n",
    "NB due to privacy concerns this notebook will rely on synthetic data, and will not replicate the full database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cef9e",
   "metadata": {},
   "source": [
    "## Switch runtime to T4\n",
    "To run this you will need to use some of your T4 hours on GoogleColab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8150f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure you have selected a T4 GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "# You can check your current GPU with:\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected. Please change your runtime type to GPU (e.g., T4 GPU).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9eca2b",
   "metadata": {},
   "source": [
    "## Setup Environment & Data\n",
    "This cell clones the repository to get the synthetic data and installs necessary libraries.\n",
    "*(Run this once. It takes about 2-3 minutes.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a51017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone repo if not already present\n",
    "if not os.path.exists(\"qwen3-vision-structured-extraction\"):\n",
    "    !git clone https://github.com/LeiweAndPartners/qwen3-vision-structured-extraction..git\n",
    "    %cd qwen3-vision-structured-extraction.\n",
    "else:\n",
    "    %cd qwen3-vision-structured-extraction.\n",
    "    !git pull\n",
    "\n",
    "# Install dependencies (Quiet mode to reduce clutter)\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git # Ensure latest Transformers for Qwen2.5/3\n",
    "!sudo apt-get install poppler-utils  # For PDF conversion\n",
    "\n",
    "print(\"âœ… Environment Ready. Data available in ./data/synthetic_samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73ce63",
   "metadata": {},
   "source": [
    "## 2. Load Qwen Model\n",
    "We use 4-bit quantization to ensure this runs on a standard Google Colab T4 GPU.\n",
    "\n",
    "NB This requires you to use a huggingface token in order to gain access to the model.\n",
    "\n",
    "---\n",
    "\n",
    "**If you don't know how to get your token follow the steps below**\n",
    "1. Go to your Hugging Face account, move to the tokens portion of the [settings](https://huggingface.co/settings/tokens)\n",
    "\n",
    "2. Click **New token** (top right or under â€œAccess Tokensâ€ section)\n",
    "    \n",
    "    In the popup:\n",
    "    - Give your token a name (e.g., colab-test, qwen-ocr)\n",
    "    - Select the scope: Read is enough for most models\n",
    "    - Click \"Generate\"\n",
    "    - Copy the token string (starts with hf_...)\n",
    "\n",
    "3. Go back to your Colab tab\n",
    "    - On the left hand side ribbon click on the **secrets** icon (should be a key icon, 4th from the top)\n",
    "    - Select \"+ add a new secret\"\n",
    "    - Specify the `NAME` as `HF_TOKEN`, and the `Value` as the token string you got in step2 (`hf.....`)\n",
    "    - Ensure the notebook has access to the HF_TOKEN variable.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# Configuration for 4-bit loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "print(\"Loading Qwen3-VL-8B-Instruct model...\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-8B-Instruct\", \n",
    "    quantization_config=bnb_config,\n",
    "    dtype=\"auto\", \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"âœ… Model Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2fabe4",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fe18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper Functions\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_document_image(filepath):\n",
    "    \"\"\"Converts PDF/Image into a PIL Image and resizes for T4 compatibility.\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # Force cleanup before loading new heavy data\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    image = None\n",
    "    \n",
    "    if ext in ['.jpg', '.jpeg', '.png']:\n",
    "        image = Image.open(filepath)\n",
    "    elif ext == '.pdf':\n",
    "        # Convert first page of PDF to image\n",
    "        # lower DPI to 150 to save memory immediately\n",
    "        images = convert_from_path(filepath, dpi=150)\n",
    "        image = images[0]\n",
    "    \n",
    "    if image:\n",
    "        # --- RESIZE LOGIC TO PREVENT OOM ---\n",
    "        max_dimension = 1024\n",
    "        w, h = image.size\n",
    "        if max(w, h) > max_dimension:\n",
    "            scale = max_dimension / max(w, h)\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "            image = image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "            print(f\"ðŸ“‰ Resized image from {w}x{h} to {new_w}x{new_h} for memory safety.\")\n",
    "            \n",
    "    return image\n",
    "\n",
    "def run_qwen_inference(image, prompt_text):\n",
    "    \"\"\"Sends image + prompt to the model.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Prepare inputs\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbac4a0",
   "metadata": {},
   "source": [
    "## 4. The pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Run Extraction Pipeline\n",
    "# @markdown This script iterates through the synthetic folder, classifies the document, and runs the specific extractor.\n",
    "\n",
    "DATA_DIR = \"./data/synthetic_samples\"\n",
    "# Filter for only the files we want to demo (Images and PDFs)\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.endswith(('.jpg', '.pdf'))]\n",
    "files.sort()\n",
    "\n",
    "# --- Prompts ---\n",
    "PROMPT_CLASSIFY = \"\"\"\n",
    "Classify this document into one of the following categories. \n",
    "Return ONLY the category name.\n",
    "Options:\n",
    "1. Immigration Recognizance Form\n",
    "2. Tenancy Agreement\n",
    "3. Other\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_EXTRACT_RECOGNIZANCE = \"\"\"\n",
    "You are a data extraction assistant. Extract the following fields into a valid JSON object.\n",
    "Ensure keys are: serial_no, name, recognizance_no, dob, address, reporting_condition_summary.\n",
    "If the document is bilingual, prefer English.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_EXTRACT_TENANCY = \"\"\"\n",
    "You are a data extraction assistant. Extract the following fields into a valid JSON object.\n",
    "Ensure keys are: landlord, tenant, monthly_rent, lease_term_months, rent_payment_date.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Found {len(files)} documents to process.\\n\")\n",
    "\n",
    "for filename in files:\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    image = load_document_image(filepath)\n",
    "    \n",
    "    if image is None:\n",
    "        continue\n",
    "        \n",
    "    print(f\"--- Processing: {filename} ---\")\n",
    "    \n",
    "    # 1. Show Image Preview\n",
    "    plt.figure(figsize=(4, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Classify\n",
    "    doc_type = run_qwen_inference(image, PROMPT_CLASSIFY)\n",
    "    print(f\"ðŸ“‚ Classification: {doc_type}\")\n",
    "    \n",
    "    # 3. Route & Extract\n",
    "    extraction_result = \"{}\"\n",
    "    if \"Recognizance\" in doc_type:\n",
    "        extraction_result = run_qwen_inference(image, PROMPT_EXTRACT_RECOGNIZANCE)\n",
    "    elif \"Tenancy\" in doc_type:\n",
    "        extraction_result = run_qwen_inference(image, PROMPT_EXTRACT_TENANCY)\n",
    "    else:\n",
    "        extraction_result = \"Skipped extraction for this type.\"\n",
    "        \n",
    "    # 4. Display Result\n",
    "    print(\"ðŸ“Š Extraction Result:\")\n",
    "    # Clean up markdown code blocks if present\n",
    "    clean_json = extraction_result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    print(clean_json)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
